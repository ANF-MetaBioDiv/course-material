---
title: "Preprocessing : dada2"
date: "August 2022"
output: html_document
author:
  - "Fabrice Armougom, MIO"
  - "Marc Garel, MIO"
editor_options: 
  chunk_output_type: inline
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warnings = FALSE)
```

## Prepare your working directory

### Get the repository files

To begin with, you will download the course repository on your computer.

To do so, visit the repository on github 
([ANF-metabarcoding](https://github.com/nhenry50/ANF-metabarcoding)) 
and download it as a zip file.

Once on your computer, unzip the file and place the resulting folder
in the most covenient location (`~/Documents` for example).

### Dowload the reference database

Save the path to the folder where you will place 
the references databases.

```{r}
refdb_folder <- here::here("data", "refdb")
refdb_folder
```

The reason why we use here::here() is that when you render a
Rmarkdown, the working directory is where the Rmarkdown file is:

```{r}
getwd()
```

Whereas here::here() point to the root of the R project

```{r}
here::here()
```

Now, let's create the folder directly from R:

```{r}
if (!dir.exists(refdb_folder)) dir.create(refdb_folder)
```

You can also create the folder from RStudio in the `Files` window

**Tip:** You can access the documentation of a function using `?`
in the console. If you to know everything about the function
`dir.create()`, simply run `?dir.create()` 

The Silva reference database, commonly used to assign 16S metabarcoding data,
will be used in practical.

In case you are working with 18S sequences, you will have better assignements
using PR2 (https://pr2-database.org/) or EukRibo (https://doi.org/10.5281/zenodo.6327891)
especially if you are interested in protists.

The following code downloads dada2 formated silva reference database. 
If you are not confortable with it, you can simply
download the reference database files from your web browser
[here]("https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz") and
[here]("https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz").

```{r}
# R stop dowloading after timeout which is
# 60 seconds by default
getOption("timeout")

# so we change timeout to be 20 minutes
options(timeout = 1200)

# we save in variable the path to the refdb
# in the working space
silva_train_set <- file.path(refdb_folder,
                             "silva_nr99_v138.1_train_set.fa.gz")

silva_species_assignment <- file.path(refdb_folder,
                                      "silva_species_assignment_v138.1.fa.gz")

# then we download the files if they don't already exist

if (!file.exists(silva_train_set)) {
  download.file(
    "https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz",
    silva_train_set,
    quiet = TRUE
  )
}

if (!file.exists(silva_species_assignment)) {
  download.file(
    "https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz",
    silva_species_assignment,
    quiet = TRUE
  )
}

```

### Attach custom functions

We will used R functions specifically written of this course.
The "classic" way is to use `source()` with the name of the R script
to source.

Instead, we use `devtools::load_all()`. This function will source all the 
script from the folder `R/` along with the documentation in `man/
`
```{r}
# Attached functions from preprocessing.R located in the R folder
devtools::load_all()
```

## Inputs files

### Locate the sequencing files

Save the path to the directory containing your raw data
(paried-end sequencing fastq files) in an object named `path_to_fastqs`

```{r}
path_to_fastqs <- here::here("data", "raw")
```

The [gzipped](https://en.wikipedia.org/wiki/Gzip) (compressed)
[FASTQ](https://en.wikipedia.org/wiki/FASTQ_format) formated 
"forward" (R1) and "reverse" (R2) files
are named as follow:

* `${SAMPLENAME}_R1.fastq.gz` for the forward files
* `${SAMPLENAME}_R2.fastq.gz` for the reverse files.

We list the forward files using the function "list.files()".
The argument `pattern` gives you the possibility to only select
file names matching a regular expression. In our case, we select
all file names finising by `_R1.fastq.gz`.

```{r}
fnFs <- sort(list.files(path_to_fastqs,
                        pattern = "_R1.fastq.gz",
                        full.names = TRUE))
```

We do the same for reverse samples.

```{r}
fnRs <- sort(list.files(path_to_fastqs,
                        pattern = "_R2.fastq.gz",
                        full.names = TRUE))
```

**To understand:** What fnFs & fnRs variables contain?

```{r}
head(fnFs)
```

### Extract sample names

```{r}
sample_names <- basename(fnFs) |>
  strsplit(split = "_") |>
  sapply(`[`, 1)
```

**To understand:**

`basename()`: remove path to only keep file name.

`|>`: R "pipe". It allows you to chain functions, avoinding intermediate variables and
nested parenthesis. It basically transfers the output of the left expression to the input
of the right expression.

`strsplit()`: split character chain according a defined pattern. `?strsplit` for documentation.

`sapply()`: apply a function to each element of a list or vector. The output is simplified to
be vector.

Let's go step by step. First list the R1 file names.

```{r}
basename(fnFs) |>
  head()
```

We can see that the sample name is before the first `_`.
With `strsplit()`, we can split the file names into a 2 elements vector.
The result is a list of 2 elements vectors.

```{r}
basename(fnFs) |>
  strsplit(split = "_") |>
  head()
```

Now, We just have to extract the first element for each file.

```{r}
basename(fnFs) |>
  strsplit(split = "_") |>
  sapply(`[`, 1) |>
  head()
```

**Tip:** you can achieve the same thing using regular expressions:

```{r}
gsub("^.+/|_.+$", "", fnFs) |> head()
```

Regular expressions are extremly useful. If you are keen to learn
how to use them, have a look here: 

## Sequence quality check

We use a custom function, `qualityprofile()`,
implemented in `R/preprocessing.R`.

Run `?qualityprofile` to know more about this function.

```{r}
# create a directory for the outputs
quality_folder <- here::here("outputs", "dada2", "quality_plots")

if (!dir.exists(quality_folder)) {
  dir.create(quality_folder, recursive = TRUE)
}

qualityprofile(fnFs,
               fnRs,
               file.path(quality_folder, "quality_plots.pdf"))
```

**Open the qualityplot.pdf**

## Primer removal

### Prepare outputs 

We first create a folder where to save the trimmed reads:

```{r}
path_to_trimmed_reads <- here::here("outputs", "dada2", "trimmed")

if (!dir.exists(path_to_trimmed_reads)) dir.create(path_to_trimmed_reads)
```

Then we prepare a list of file names with path where to save the results
of the primer removal (*e.g.* sequences without primers)

```{r}
nopFw <- file.path(path_to_trimmed_reads, basename(fnFs))
nopRv <- file.path(path_to_trimmed_reads, basename(fnRs))

head(nopFw)
```

### Remove primers
```{r}
primer_fwd  <- "CCTACGGGNBGCASCAG"
primer_rev  <- "GACTACNVGGGTATCTAAT"
```

Remove primers from forward `fnFs` and reverse `fnRs` reads 
and save the result in `nopFs` and `nopRs` respectively.

```{r}
dada2::removePrimers(fn = fnFs,
                     fout = nopFw,
                     primer.fwd = primer_fwd,
                     max.mismatch = 1,
                     verbose = TRUE)

dada2::removePrimers(fn = fnRs,
                     fout = nopRv,
                     primer.fwd = primer_rev,
                     max.mismatch = 1,
                     verbose = TRUE)
```

### For more complex situations

If you have to deal with lots of samples and/or mixed orientated reads,
we would recommend using dedicated tools such as [cutadapt](https://github.com/marcelm/cutadapt).

If you have to deal with mix-orientated paired-end reads,
you may find inspiration [here](https://gitlab.sb-roscoff.fr/nhenry/abims-metabarcoding-pipeline-dada2)

## Trimming and quality filtering

#### Prepare outputs

Same as before, create a folder

```{r}
path_to_filtered_reads <- here::here("outputs", "dada2", "filtered")
if (!dir.exists(path_to_filtered_reads)) dir.create(path_to_filtered_reads)
```

and list file names with path

```{r}
filtFs <- file.path(path_to_filtered_reads, basename(fnFs))
filtRs <- file.path(path_to_filtered_reads, basename(fnRs))
names(filtFs) <- sample_names
names(filtRs) <- sample_names
```

#### Use `dada2::filterAndTrim()`

Let's have a look at what the function is doing.
To do so, type `?dada2::filterAndTrim()` in the console

```{r}
(out <- dada2::filterAndTrim(
  fwd = nopFw,
  filt = filtFs,
  rev = nopRv,
  filt.rev = filtRs,
  minLen = 150,
  matchIDs = TRUE,
  maxN = 0,
  maxEE = c(3, 3),
  truncQ = 2
))
```

#### Details

* **nopFw** : input, where the forward reads without primers are (path)
* **filtFs** : output, where forward filtered reads are written (path)
* **nopRv** and **filRs** : same as above, but wiht reverse reads
* **TruncLen** : truncate reads after truncLen bases.
Reads shorter than that are discarded.
Exple : TruncLen=c(200,150), means forward and reverse reads are cut
at 200 bp and 150 bp respectively.
* **TrimLeft** : number of nucleotides to remove from the start
* **Trimright** : number of nucleotides to remove from the end
* **maxN** : max number of ambiguous bases accepted
* **maxEE** : read expected errors (EE) threshold.
The EE of a read is the sum of the error probability
of each base composing it.
Increase that value to accept more low quality reads.
The first value refers to the forward reads and
the second to the reverse reads.
* **TruncQ=2**: truncate reads at the first instance of a
quality score less than or equal to truncQ.

## Denoising
### Learn the error model
```{r}
errF <- dada2::learnErrors(filtFs,
                           randomize = TRUE,
                           multithread = TRUE)

errR <- dada2::learnErrors(filtRs,
                           randomize = TRUE,
                           multithread = TRUE)
```

You can visualise the resulting error model using
the function `dada2::plotErrors()`
```{r}
dada2::plotErrors(errF, nominalQ=TRUE)
```

### Dereplication

Let's have a look at the function's documentation: `?dada2::derepFastq()`

```{r}
derepFs <- dada2::derepFastq(filtFs, verbose = TRUE)

derepRs <- dada2::derepFastq(filtRs, verbose = TRUE)
```

### Run dada

Let's have a look at the function's documentation: `?dada2::dada()`

```{r}
dadaFs <- dada2::dada(derepFs, err = errF, multithread = TRUE)

dadaRs <- dada2::dada(derepRs, err = errR, multithread = TRUE)
```

## Merge paired-end reads

```{r}
mergers <- dada2::mergePairs(
  dadaF = dadaFs,
  derepF = derepFs,
  dadaR = dadaRs,
  derepR = derepRs,
  maxMismatch = 0,
  verbose = TRUE
)
```

## Build the ASV table

```{r}
seqtab <- dada2::makeSequenceTable(mergers)
```

## Remove Chimera

```{r}
seqtab_nochim <- dada2::removeBimeraDenovo(seqtab,
                                           method = "consensus",
                                           multithread = TRUE,
                                           verbose = TRUE)
```

## Summarize the pre-processing

```{r}
getN <- function(x) sum(dada2::getUniques(x))

track <- data.frame(input = out[, 1],
                    filtered = out[, 2],
                    denoisedF = sapply(dadaFs, getN),
                    denoisedR = sapply(dadaRs, getN),
                    merged = sapply(mergers, getN),
                    nonchim = rowSums(seqtab_nochim),
                    perc_retained = rowSums(seqtab_nochim)/out[,1]*100)

rownames(track) <- sample_names

track
```

## Taxonomic assignment from dada2

The taxonomic assignment can be achieved using the functions
`assignTaxonomy()` and `addSpecies()` from dada2.

```{r}
taxonomy <- dada2::assignTaxonomy(
  seqs = seqtab_nochim,
  refFasta = silva_train_set,
  taxLevels = c("Kingdom", "Phylum", "Class",
                "Order", "Family", "Genus",
                "Species"),
  multithread = TRUE,
  minBoot = 60
)

taxonomy <- dada2::addSpecies(taxonomy,
                          silva_species_assignment,
                          allowMultiple = FALSE)
```

## Export ASV table

### R objects

```{r}
export_folder <- here::here("outputs", "dada2", "asv_table")

if (!dir.exists(export_folder)) dir.create(export_folder)

saveRDS(object = seqtab_nochim,
        file = file.path(export_folder, "seqtab_nochim.rds"))

saveRDS(object = taxonomy,
        file = file.path(export_folder, "taxonomy.rds"))
```

### Text files

```{r}
asv_seq <- colnames(seqtab_nochim)
```

```{r}
asv_id <- sapply(asv_seq,
                 function(x) unname(digest::digest(x, algo = "sha1")))
```

```{r}
row.names(taxonomy) <- colnames(seqtab_nochim) <- names(asv_seq) <- asv_id
```

```{r}
taxonomy <- df_export(taxonomy, new_rn = "asv")

write.table(taxonomy,
            file = file.path(export_folder, "taxonomy.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

```{r}
seqtab_nochim <- t(seqtab_nochim)

seqtab_nochim <- df_export(seqtab_nochim, new_rn = "asv")

write.table(seqtab_nochim,
            file = file.path(export_folder, "asv_table.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```

```{r}
cat(paste0(">", names(asv_seq), "\n", asv_seq),
    sep = "\n",
    file = file.path(export_folder, "asv.fasta"))
```