---
title: "Preprocessing : dada2"
date: "August 2022"
output: html_document
author:
  - "Fabrice Armougom, MIO"
  - "Marc Garel, MIO"
editor_options: 
  chunk_output_type: inline
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warnings = FALSE)
```

## Prepare your working space

### Get the files

To begin with, you will download the course repository on your computer.

On way is to go to the repository on github 
([ANF-metabarcoding](https://github.com/nhenry50/ANF-metabarcoding)) 
and to download it as a zip file.

Once on your computer, unzip the file and place the resulting folder
in the most covenient location (`~/Documents` for example)

### Dowload the reference database

First we save in an object the path to the folder where you will place 
the references databases.

```{r}
refdb_folder <- here::here("data", "refdb")
refdb_folder
```

The reason why we use here::here() is that when you render a
Rmarkdown, the working directory is where the Rmarkdown file is:

```{r}
getwd()
```

Whereas here::here() point to the root of the R project

```{r}
here::here()
```

Now, let's create the folder directly from R:

```{r}
if (!dir.exists(refdb_folder)) dir.create(refdb_folder)
```

You can also create the folder from RStudio in the `Files` window

You can access the documentation of a function using `?`
in the console. If you to know everything about the function
`dir.create()`, simply run `?dir.create()` 

In this practical, we are analysing 16S data.
Silva is a well suited reference database for Prokaryote.

In case you are working on 18S sequences, it is better to
use PR2 (ref)

```{r}
# R stop dowloading after timeout which is
# 60 seconds by default
getOption('timeout')

# so we change timeout to be 20 minutes
options(timeout=1200)

# we save in variable the path to the refdb
# in the working space
silva_train_set <- file.path(refdb_folder,"silva_nr99_v138.1_train_set.fa.gz")
silva_species_assignment <- file.path(refdb_folder,"silva_species_assignment_v138.1.fa.gz")

# then we download the files if they don't
# already exist
# note: if you copy/paste in your web browser
# it is also working
if(!file.exists(file.path(refdb_folder,"silva_nr99_v138.1_train_set.fa.gz"))){
  download.file("https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz",
              silva_train_set,
              quiet=TRUE)
}

if(!file.exists(file.path(refdb_folder,"silva_species_assignment_v138.1.fa.gz"))){
  download.file("https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz",
              silva_species_assignment,
              quiet=TRUE)
}

```

### Attach custom functions

```{r}
# Attached functions from preprocessing.R located in the R folder
devtools::load_all()
```

## Sequencing files

### List of sequencing file paths

Save the path to the directory containing your raw data
(paried-end sequencing fastq files) in an object named `path_to_fastqs`

```{r}
path_to_fastqs <- here::here("data", "raw")
```

The "forward" and "reverse" files are in gzipped (compressed)
fastq format with the label:

* `${SAMPLENAME}_R1.fastq.gz` for the forward files
* `${SAMPLENAME}_R2.fastq.gz` for the reverse files.

R1 for forward reads and R2 for reverse reads.

```{r}
fnFs <- sort(list.files(path_to_fastqs,
                        pattern = "_R1.fastq.gz",
                        full.names = TRUE))[1:2]

fnRs <- sort(list.files(path_to_fastqs,
                        pattern = "_R2.fastq.gz",
                        full.names = TRUE))[1:2]
```

**To understand: What fnFs & fnRs variables contain?**

```{r}
fnFs |> head()
```

### Extract sample names

```{r}
(sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1))
```

**To understand:**

`strsplit()` : split character chain according a defined pattern. `?strsplit` for function documentation.

`basename()`: remove path to only keep file name

```{r}
strsplit(basename(fnFs), "_") |> head()
```

```{r}
sapply(strsplit(basename(fnFs), "_"), `[`, 2) |> head()
```

```{r}
sapply(strsplit(basename(fnFs), "_"), `[`, 3) |> head()
```

With regular expressions:

```{r}
gsub("^.+/|_.+$", "", fnFs) |> head()
```

## Sequence quality check

We use a custom function, `qualityprofile()`,
implemented in `R/preprocessing.R`.

Run `?qualityprofile` to know more about this function.

```{r}
# create a directory for the outputs
quality_folder <- here::here("outputs", "dada2", "quality_plots")

if (!dir.exists(quality_folder)) {
  dir.create(quality_folder, recursive = TRUE)
}

qualityprofile(fnFs,
               fnRs,
               file.path(quality_folder, "quality_plots.pdf"))
```

**Open the qualityplot.pdf**

## Primer removal

### Prepare outputs 

We first create a folder where to save the trimmed reads:

```{r}
path_to_trimmed_reads <- here::here("outputs", "dada2", "trimmed")

if (!dir.exists(path_to_trimmed_reads)) dir.create(path_to_trimmed_reads)
```

Then we prepare a list of file names with path where to save the results
of the primer removal (*e.g.* sequences without primers)

```{r}
nopFw <- file.path(path_to_trimmed_reads, basename(fnFs))
nopRv <- file.path(path_to_trimmed_reads, basename(fnRs))

head(nopFw)
```

### Remove primers
```{r}
primer_fwd  <- "CCTACGGGNBGCASCAG"
primer_rev  <- "GACTACNVGGGTATCTAAT"
```

Remove primers from forward `fnFs` and reverse `fnRs` reads 
and save the result in `nopFs` and `nopRs` respectively.

```{r}
dada2::removePrimers(fn = fnFs,
                     fout = nopFw,
                     primer.fwd = primer_fwd,
                     max.mismatch = 1,
                     verbose = TRUE)

dada2::removePrimers(fn = fnRs,
                     fout = nopRv,
                     primer.fwd = primer_rev,
                     max.mismatch = 1,
                     verbose = TRUE)
```

### For more complex situations

If you have to deal with lots of samples and/or mixed orientated reads,
we would recommend using dedicated tools such as [cutadapt](https://github.com/marcelm/cutadapt).

If you have to deal with mix-orientated paired-end reads,
you may find inspiration [here](https://gitlab.sb-roscoff.fr/nhenry/abims-metabarcoding-pipeline-dada2)

## Trimming and quality filtering

#### Prepare outputs

Same as before, create a folder

```{r}
path_to_filtered_reads <- here::here("outputs", "dada2", "filtered")
if (!dir.exists(path_to_filtered_reads)) dir.create(path_to_filtered_reads)
```

and list file names with path

```{r}
filtFs <- file.path(path_to_filtered_reads, basename(fnFs))
filtRs <- file.path(path_to_filtered_reads, basename(fnRs))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

#### Use `dada2::filterAndTrim()`

Let's have a look at what the function is doing.
To do so, type `?dada2::filterAndTrim()` in the console

```{r}
(out <- dada2::filterAndTrim(
  fwd = nopFw,
  filt = filtFs,
  rev = nopRv,
  filt.rev = filtRs,
  minLen = 150,
  matchIDs = TRUE,
  maxN = 0,
  maxEE = c(3, 3),
  truncQ = 2
))
```

#### Details

* **nopFw** : input, where the forward reads without primers are (path)
* **filtFs** : output, where forward filtered reads are written (path)
* **nopRv** and **filRs** : same as above, but wiht reverse reads
* **TruncLen** : truncate reads after truncLen bases.
Reads shorter than that are discarded.
Exple : TruncLen=c(200,150), means forward and reverse reads are cut
at 200 bp and 150 bp respectively.
* **TrimLeft** : number of nucleotides to remove from the start
* **Trimright** : number of nucleotides to remove from the end
* **maxN** : max number of ambiguous bases accepted
* **maxEE** : read expected errors (EE) threshold.
The EE of a read is the sum of the error probability
of each base composing it.
Increase that value to accept more low quality reads.
The first value refers to the forward reads and
the second to the reverse reads.
* **TruncQ=2**: truncate reads at the first instance of a
quality score less than or equal to truncQ.

## Denoising
### Learn the error model
```{r}
errF <- dada2::learnErrors(filtFs,
                           randomize = TRUE,
                           multithread = TRUE)

errR <- dada2::learnErrors(filtRs,
                           randomize = TRUE,
                           multithread = TRUE)
```

You can visualise the resulting error model using
the function `dada2::plotErrors()`
```{r}
dada2::plotErrors(errF, nominalQ=TRUE)
```

### Dereplication

Let's have a look at the function's documentation: `?dada2::derepFastq()`

```{r}
derepFs <- dada2::derepFastq(filtFs, verbose = TRUE)

derepRs <- dada2::derepFastq(filtRs, verbose = TRUE)
```

### Run dada

Let's have a look at the function's documentation: `?dada2::dada()`

```{r}
dadaFs <- dada2::dada(derepFs, err = errF, multithread = TRUE)

dadaRs <- dada2::dada(derepRs, err = errR, multithread = TRUE)
```

## Merge paired-end reads

```{r}
mergers <- dada2::mergePairs(
  dadaF = dadaFs,
  derepF = derepFs,
  dadaR = dadaRs,
  derepR = derepRs,
  maxMismatch = 0,
  verbose = TRUE
)
```

## Build the ASV table

```{r}
seqtab <- dada2::makeSequenceTable(mergers)
```

## Remove Chimera

```{r}
seqtab.nochim <- dada2::removeBimeraDenovo(seqtab,
                                           method = "consensus",
                                           multithread = TRUE,
                                           verbose = TRUE)
```

## Summarize the pre-processing

```{r}
getN <- function(x) sum(dada2::getUniques(x))

track <- data.frame(input = out[, 1],
                    filtered = out[, 2],
                    denoisedF = sapply(dadaFs, getN),
                    denoisedR = sapply(dadaRs, getN),
                    merged = sapply(mergers, getN),
                    nonchim = rowSums(seqtab.nochim),
                    perc_retained = rowSums(seqtab.nochim)/out[,1]*100)

rownames(track) <- sample.names

track
```

## Taxonomic assignment from dada2

The taxonomic assignment can be achieved using the functions
`assignTaxonomy()` and `addSpecies()` from dada2.

```{r}
taxonomy <- dada2::assignTaxonomy(
  seqs = seqtab.nochim,
  refFasta = silva_train_set,
  taxLevels = c("Kingdom", "Phylum", "Class",
                "Order", "Family", "Genus",
                "Species"),
  multithread = TRUE,
  minBoot = 60
)

taxonomy <- dada2::addSpecies(taxonomy,
                          silva_species_assignment,
                          allowMultiple = FALSE)
```

## Export ASV table

### R objects

```{r}
export_folder <- here::here("outputs", "dada2", "asv_table")

if (!dir.exists(export_folder)) dir.create(export_folder)

saveRDS(object = seqtab.nochim,
        file = file.path(export_folder, "seqtab_nochim.rds"))

saveRDS(object = taxonomy,
        file = file.path(export_folder, "taxonomy.rds"))
```

### Text files

```{r}
taxo_export <- apply(taxonomy, 1, paste, collapse=";")
seqtab_export <- t(seqtab.nochim)

seqtab_export <- merge(x = taxo_export,
      y = seqtab_export,
      by = 0,
      all.x = TRUE,
      sort = FALSE)

colnames(seqtab_export)[1:2] <- c("sequence", "taxonomy")

write.table(seqtab_export,
            file = file.path(export_folder, "asv_table.tsv"),
            quote = FALSE,
            sep = "\t",
            row.names = FALSE)
```
