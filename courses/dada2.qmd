---
title: "Analysing metabarcoding data using dada2"
author:
  - name: Nicolas Henry
    affiliations:
    orcid: 0000-0002-7702-1382
    email: nicolas.henry@cnrs.fr
format:
  revealjs:
    slide-number: true
    theme: night
    embed-resources: true
bibliography: ../references.bib
---

# Introduction

## Metabarcoding at the lab

![](../img/metaB_lab_1.png)

## Sequencing generations

<br>

![](../img/sequencing.png)

## Paired-end sequencing

<br>

![](../img/paired_end_seq.png)

::: aside
Short reads: from 2\*150 bp to 2\*300 bp, often 2\*250 bp
:::

## Next steps until an ASV table

::: r-stack
![](../img/metaB_lab_2.png)

![](../img/metaB_lab_3.png){.fragment}
:::

# FASTQ records

## Overview

Two files (R1 and R2) per sequencing run or per sample (100 first nucleotides):

R1:

```{r}
here::here("data","raw","S11B_R1.fastq.gz") |>
    gzfile() |>
    readLines(n = 4) |>
    substr(1,100) |>
    cat(sep="\n")

```

R2:

```{r}
here::here("data","raw","S11B_R2.fastq.gz") |>
    gzfile() |>
    readLines(n = 4) |>
    substr(1,100) |>
    cat(sep="\n")
```

Per record: identifier, sequence, quality

## Identifier

![](../img/fastq_id.png)

## Sequence

<br>

![](../img/read_example_1.png)

-   The tag informs you from which sample the read come from

-   The primer used for the amplification

-   The targeted sequence (metabarcode)

## Quality

Phred quality score (Q) encoded using ASCII characters:

<br>

![](../img/score_encoding.png)

<br>

$Q$ is related to the base calling error probability ($P$):

$$Q = -10\log_{10}P$$

$$P=10^{Q/-10}$$

::: notes
In raw data reads, the quality score rarely exceed 40, but higher scores are possible when different raw reads are assembled
:::

## Quality: $P$ as a function of $Q$

```{r}
library(ggplot2)
library(patchwork)

score_table <- data.frame(
  a = rawToChar(as.raw(0:40+33), multiple = TRUE),
  b = 10^((0:40)/-10)
)

score_table$a <- factor(score_table$a, levels = score_table$a)

p1 <- ggplot(score_table, aes(x=a,y=b))+
  geom_line(group = 1) +
  scale_x_discrete("ASCII encoded Phred quality score") +
  scale_y_continuous(labels = scales::label_percent(), "Base calling error probability") +
  theme_bw()

p2 <- ggplot(score_table, aes(x=a,y=b))+
  geom_line(group = 1) +
  scale_x_discrete("ASCII encoded Phred quality score") +
  scale_y_log10(labels = scales::label_percent(), "Base calling error probability") +
  theme_bw()

p1 + p2

```

# Bioinformatic pipelines

## Main strategies

::: r-stack
![](../img/clust_denois_1.png)

![](../img/clust_denois_2.png){.fragment}

![](../img/clust_denois_3.png){.fragment}

![](../img/clust_denois_4.png){.fragment}
:::

::: aside
<small> adapted from [@antich2021denoise] </small>
:::

## Some tools

<br>

-   Genetic clustering based approaches producing OTUs:
    -   [UPARSE](https://doi.org/10.1038/nmeth.2604)
    -   [Swarm](https://doi.org/10.1093/bioinformatics/btab493) (could be seen as denoising too)
-   Denoising approaches producing ESVs:
    -   [Deblur](https://doi.org/10.1128/mSystems.00191-16) (sOTUs)
    -   [DADA2](https://doi.org/10.1038%2Fnmeth.3869) (ASVs)

## Many tools

![](../img/many_tools.png){height="500" fig-pos="center"}

::: aside
<small> Extracted from [@hakimzadeh2023pile] </small>
:::

## Workflow manager

[Nextflow](https://www.nextflow.io/index.html) and [Snakemake](https://snakemake.github.io/) are the most popular

![](https://raw.githubusercontent.com/nf-core/ampliseq/2.6.1//docs/images/ampliseq_workflow.png){height="500" fig-pos="center"}

## DADA2, qu'est-ce que c'est?

<br>

-   DADA (Divisive Amplicon Denoising Algorithm) an algorithm to denoise Roche's 454 platform errors [@rosen2012denoising]

-   DADA2 implements a new quality-aware model of Illumina amplicon errors [@callahan2016dada2]

-   DADA2 is an open-source R package <https://github.com/benjjneb/dada2>

## DADA2, the full workflow

-   Quality assessment → `plotQualityProfile()`
-   Length trimming → `filterAndTrim()`
-   Quality filtering → `filterAndTrim()`
-   Denoising → `dada()`
-   Merging pair-end reads → `mergePairs()`
-   Chimera identification → `removeBimeraDenovo()`
-   Taxonomic assignment → `assignTaxonomy()`

# Before using Dada2

## Tags and primers

![](../img/read_example_1.png)

**Tags** are used to encode the sample provenance of the reads. Reads need to be grouped by sample provenance (demultiplexing)

**Primer** sequences will bias the error model built by DADA2 and need to be removed (primer trimming).

Both task can be achieved using **Cutadapt**, a command line-tool to find and remove error-tolerantly adapters from reads [@martin2011cutadapt].

## Demultiplexing using cutadapt

If your tags are in a fasta file with corresponding sample names as header, you can use this command-line:

::: columns
::: {.column width="50%"}
``` bash
cutadapt \
    -g file:${BARCODES} \
    -o {name}_R1.fastq.gz \
    -p {name}_R2.fastq.gz \
    ${R1_FILE} \
    ${R2_FILE}
```
:::

::: {.column width="50%"}
``` bash
# 
# tags to look for at the beginning (5') of R1 files. ${BARCODES} is a fasta file containing the tags
# demultiplexed R1 file output. name will be replace by the name of the tag
# same as above but with R2 files
# input R1 file
# input R2 file
```
:::
:::

As many R1 and R2 files as samples you have

::: callout-warning
## Help! My reads are mixed-orientated

Run `cutadapt` a second time, looking for tags in R2.

Keep the outputs of the two rounds separated for the rest of the workflow.
:::

## Primer removal using cutadapt

To remove forward and reverse primer sequences from pair-end read files:

::: columns
::: {.column width="50%"}
``` bash
cutadapt \
    -e 0.1 \
    -g ${PRIMER_FORWARD} \
    -G ${PRIMER_REVERSE} \
    --report=minimal \
    --discard-untrimmed \
    --minimum-length ${MIN_LENGTH} \
    --no-indels \
    -o ${FORWARD} \
    -p ${REVERSE} \
    ${R1_FILE} \
    ${R2_FILE} 1> ${LOG}
```
:::

::: {.column width="50%"}
``` bash
# 
# error tolerance (default value)
# forward primer
# reverse primer
# ask to print primer trimming statistics
# reads not containing primers are discarded
# read with a length below this threshold after trimming are discarded
# no indels allowed when mathing primer to reads
# R1 output
# R2 output
# R1 input
# R2 input; 1> ${LOG} export the report in the file ${LOG}
```
:::
:::

As for demultiplexing, if reads are mix-orientated, run `cutadapt` twice

# DADA2 workflow: reads preparation

## Check reads quality

::: columns
::: {.column width="50%"}
Check the overall quality of a sequencing run using `plotQualityProfile()`

Outside of DADA2, you can also use <a href="https://github.com/s-andrews/FastQC" target="_blank">FASTQC</a>

If the overall quality is too low, you will probably have to resequence your samples
:::

::: {.column width="50%"}
![](../img/quality_plot_example.svg)
:::
:::

A quality drop is often observed in the end of the reads

::: notes
-   green line: mean
-   orange line (plain): median
-   orange line (dashed): 25th and 75th quantiles
:::

## Trimming and filtering

Trimming, at a given length, will improve the overall read quality

::: callout-caution
# Danger zone

After trimming, make sure that forward and reverse reads are still long enough to overlap
:::

Reads of low quality and/or with ambiguous nucleotides (N) after trimming are filtered out.

Both length trimming and quality filtering are achieved using the function `filterAndTrim()`

# DADA2 workflow: denoising approach

## Denoising

Is sequence $i$ generated by sequence $j$ because of sequencing errors?

![](../img/alignment_example_1.png)

In order to define if $i$ is an error of $j$ and perform denoising using DADA2, we need to compute:

-   the error rate $\lambda_{ji}$
-   the abundance p-value $p_A(j \rightarrow i)$

## Error rate

> The rate at which an amplicon read with sequence i is produced from sample sequence j is reduced to the product over the transition probabilities between the L aligned nucleotides:

$$\lambda_{ji} = \prod_{l=0}^L p(j(l) \rightarrow i(l),q_i(l))$$

## The abundance p-value

The abundance p-value ($p_A$) is the probability of all reads of $i$ ($a_i$) or more being produced from $j$ during amplification or sequencing.

$$p_A(j \rightarrow i) = \frac{1}{1- \rho_{\mathrm{pois}}(n_j\lambda_{ji},0)} \sum_{a=a_i}^\infty \rho_{\mathrm{pois}}(n_j\lambda_{ji},a)$$

A low p-value indicate that it is unlikely that $i$ is noise from amplifying and sequencing $j$

::: notes
The discrete nature of the Poisson distribution is also why this is a probability mass function and not a density function
:::

## The divisive partitioning algorithm: step 1

![](../img/dada_algo_1.png){fig-pos="center" height="400"}

## The divisive partitioning algorithm: step 2

![](../img/dada_algo_2.png){fig-pos="center" height="400"}

::: aside
$p_A$ of all sequences against most abundant sequence (center)
:::

## The divisive partitioning algorithm: step 3

![](../img/dada_algo_3.png){fig-pos="center" height="400"}

::: aside
if smallest p-value below $\Omega_A$: new partition
:::

## The divisive partitioning algorithm: step 4

![](../img/dada_algo_4.png){fig-pos="center" height="400"}

::: aside
$p_A$ of all sequences against center of new partition
:::

## The divisive partitioning algorithm: step 5

![](../img/dada_algo_5.png){fig-pos="center" height="400"}

::: aside
Every sequence join the partition most likely to have produced it. Repeat steps 3 to 5 until all abundance p-values are greater than $\Omega_A$
:::

# DADA2 workflow: denoising in practice

## Learn the error model

How do we compute $\lambda_{ji}$ if we don't know the error rate for each possible transition?

The error rates will be learned from the data using `learnErrors()`

> The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution

## Visualise the error model

You can visualise the estimated error rates using the function `plotErrors()`

![](../img/error_plot.svg){fig-pos="center"}

## Run the DADA2 algorithm

-   After dereplicating your sequences (`derepFastq()`), denoise using the function `dada()`

-   By default sample inference is performed on each sample individually (`pool = FALSE`).

-   If you are interested in rare variants present in several samples use `pool = TRUE`

-   When working on big data, `pool = "pseudo"` is an interesting alternative to `pool = TRUE`

# DADA2 workflow: build the ASV table

## Merge paired reads

Merge forward and reverse reads using `mergePairs()`

![](../img/merging_example.png)

-   `minOverlap`: minimum size of overlap
-   `maxMismatch`: maximum number of mismatches
-   `justConcatenate`: in case your reads don't overlap

## Chimeras

Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant "parent" sequences.

![](../img/chimeras.png)

::: aside
Find and remove bimeras (two-parent chimeras) using the function `removeBimeraDenovo()`, per sample (`method="per-sample"`), for the entire dataset (`method="pooled"`) or in a consensus way (`method="consensus"`)
:::

::: notes
Chimeras are artifact sequences formed by two or more biological sequences incorrectly joined together. This often occurs during PCR reactions using mixed templates (i.e., uncultured environmental samples). Incomplete extensions during PCR allow subsequent PCR cycles to use a partially extended strand to bind to the template of a different, but similar, sequence. This partially extended strand then acts as a primer to extend and form a chimeric sequence. Once created, the chimeric sequence is then further amplified in subsequent cycles. The end result is a PCR artifact that does not represent a sequence that exists in nature.
:::

# Assign taxonomy

## Reference sequence database

Make the link in between nucleic sequences and taxonomic

Broad reference database: [GenBank](https://www.ncbi.nlm.nih.gov/genbank/)

-   Some specialised references databases:
    -   [Silva](https://www.arb-silva.de/) (SSU and LSU)
    -   [PR2](https://pr2-database.org/) (18S eukaryotes and 16S Chloroplastes)
    -   [MIDORI2](https://www.reference-midori.info/) (mitochondrial DNA)
    -   [Barcode Of Life Data](https://boldsystems.org/) (COI / ITS / rbcL & matK)
    -   [UNITE](https://unite.ut.ee/) (ITS)

## BLAST like approaches

Looking for the closest reference sequences to inherit their taxonomy

Best hits or LCA approaches

Recommand global alignment (usearch_global [VSEARCH](https://github.com/torognes/vsearch) command) with specialised reference database

::: columns
::: {.column width="50%"}
``` bash
vsearch \
  --usearch_global ${INPUT} \
  -db ${REFDB} \
  --id 0.80 \
  --maxaccepts 0 \
  --top_hits_only \
  --userout ${TMP} \
  --userfields "query+id1+target"
```
:::

::: {.column width="50%"}
``` bash
# 
# sequences to assign fasta file
# reference database reference database
# lower similarity threshold
# maximum hits (0 = unlimited)
# keep only best hits
# output file
# fields to export
```
:::
:::

## Classifiers (machine learning)

-   Sequence taxonomic classification based on k-mers composition.
-   No sequence similarity threshold but a confidence score instead.
-   RDP classifier (naïve Bayes method) is very popular, give good results at least for 16S/18S
-   IDTAXA, more recent, would tend to less overclassify

::: aside
DADA2 function `assignTaxonomy()` use a native implementation of the naïve Bayesian classifier method (<a href="https://doi.org/10.1128/aem.00062-07" target="_blank">Wang et al. 2007</a>).
:::

## Phylogenetic placement

![](../img/placements.png){height="500" fig-pos="center"}

::: aside
<small> Figure from https://itol.embl.de/ </small>
:::

# Extra steps

## Post-clustering with LULU

Abundance aware clustering:

![](../img/lulu.png)

::: aside
<small> Figure from [@froslev2017algorithm]. For a C++ implementation, have a look at [MUMU](https://github.com/frederic-mahe/mumu)</small>
:::

## Decontamination

Frequency-based contaminant identification with the R package [decontam](https://github.com/benjjneb/decontam):

![](../img/decontam.png)

::: aside
<small> Decontamination can also be achieved with the R package [metabaR](https://github.com/metabaRfactory/metabaR) and the function `contaslayer()`</small>
:::

# Now it is your turn!

# References
